{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d02ccae-8a3f-43d9-b262-3f765e9cc39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detected a spark instance!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set display settings!\n",
      "installed all keras and tensorflow dependencies for the LSTM model!\n",
      "intialized a spark context!\n",
      "all dependencies installed!\n"
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import uvicorn\n",
    "\n",
    "# find and init the spark instance to ensure it is pip installed\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "print(\"detected a spark instance!\")\n",
    "\n",
    "# set some HTML display setting \n",
    "from IPython.core.display import HTML\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))\n",
    "\n",
    "print(\"set display settings!\")\n",
    "# import all the pyspark dependencies \n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, StringType, DateType\n",
    "from pyspark.sql.functions import *\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "print(\"installed all keras and tensorflow dependencies for the LSTM model!\")\n",
    "\n",
    "# declare a spark object that we will run our spark SQL dataframes on \n",
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
    "\n",
    "# init a spark session \n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"intialized a spark context!\")\n",
    "\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# import libraries for data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "print(\"all dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87771fa3-8c07-49b6-a354-e770fd786208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://192.168.1.62:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [13/Oct/2023 08:45:58] \"GET /predict HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Company_Common_Name': 'Covestro AG', 'Economic_Sector_Name': 'Basic Materials', 'Business_Sector_Name': 'Chemicals', 'Industry_Group_Name': 'Chemicals', 'Industry_Name': 'Commodity Chemicals', 'Activity_Name': 'Plastics'}\n",
      "1/1 [==============================] - 0s 194ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [13/Oct/2023 08:46:03] \"POST /predict HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Company_Common_Name': '1&1 AG', 'Economic_Sector_Name': 'Technology', 'Business_Sector_Name': 'Telecommunications Services', 'Industry_Group_Name': 'Telecommunications Services', 'Industry_Name': 'Wireless Telecommunications Services', 'Activity_Name': 'Wireless Telecoms Service Providers'}\n",
      "1/1 [==============================] - 0s 18ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [13/Oct/2023 08:46:27] \"POST /predict HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [13/Oct/2023 08:46:55] \"GET /predict HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, render_template, request\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from pyspark.ml.feature import MinMaxScaler, VectorAssembler, StandardScaler, RobustScaler\n",
    "import tensorflow as tf\n",
    "from pyspark.sql.window import Window\n",
    "from tensorflow.keras.models import load_model\n",
    "import io\n",
    "import base64\n",
    "import json \n",
    "    \n",
    "app = Flask(__name__)\n",
    "model = load_model('LSTM_model_build_v0.h5')\n",
    "\n",
    "def build_pipeline(df, features):\n",
    "    df_modelSet = df.select(\n",
    "        col(\"Instrument\").cast(StringType()).alias(\"Instrument\"),\n",
    "        col(\"Year\").cast(IntegerType()).alias(\"Year\"),\n",
    "        col(\"Month\").cast(IntegerType()).alias(\"Month\"),\n",
    "        col(\"Avg_Price_Open\").cast(DoubleType()).alias(\"Avg_Price_Open\"),\n",
    "        col(\"Avg_Price_Close\").cast(DoubleType()).alias(\"Avg_Price_Close\"),\n",
    "        col(\"Avg_Price_High\").cast(DoubleType()).alias(\"Avg_Price_High\"),\n",
    "        col(\"Avg_Daily_Return\").cast(DoubleType()).alias(\"Avg_Daily_Return\")\n",
    "        )\n",
    "\n",
    "    df_modelSet = df_modelSet.filter(\n",
    "        ((col(\"Year\") == 2022) & (col(\"Month\") >= 9)) |\n",
    "        ((col(\"Year\") == 2023) & (col(\"Month\") <= 9))\n",
    "    ).sort('Year','Month')\n",
    "\n",
    "    vector_assembler = VectorAssembler(inputCols=features, outputCol=\"feature_vectors\")\n",
    "    df_assembled = vector_assembler.transform(df_modelSet)\n",
    "    \n",
    "    r_scaler = RobustScaler(inputCol=\"feature_vectors\", outputCol=\"scaled_features\", withScaling=True, withCentering=True)\n",
    "    \n",
    "    df_scaled = r_scaler.fit(df_assembled).transform(df_assembled)\n",
    "\n",
    "    w1 = Window.partitionBy(\"Instrument\").orderBy(\"Year\", \"Month\")\n",
    "    sequence_length_val = 12\n",
    "\n",
    "    # # Use Window function to create sequences\n",
    "    # #window_spec = Window.partitionBy(\"Instrument_Encoded\").orderBy(\"Year\", \"Month\")\n",
    "    for i in range(1, sequence_length_val + 1):\n",
    "        lag_col = \"lag_{}\".format(i)\n",
    "        df_scaled = df_scaled.withColumn(lag_col, lag(df_scaled[\"feature_vectors\"], i).over(w1))\n",
    "           \n",
    "    return df_scaled\n",
    "\n",
    "\n",
    "@app.route('/predict', methods=['GET','POST'])\n",
    "def predict():\n",
    "    if request.method == 'POST':\n",
    "        _dict = {}\n",
    "        ric = request.form['text_input']\n",
    "        \n",
    "        df = spark.read.csv('kaggle_stock_data_transformed.csv', header=True)\n",
    "        df = df.drop('Instrument9').withColumnRenamed('Instrument0','Instrument')\n",
    "        df_pipeline = build_pipeline(df,['Avg_Price_Open','Avg_Price_Close'])\n",
    "        df_ric = df_pipeline.filter(col('Instrument')==ric)\n",
    "        df_last = df_ric.filter((col('Year')==2023) & (col('Month')==9))\n",
    "\n",
    "        sector_data = df.filter(col('Instrument')==ric).first()  # Get the first (and only) row\n",
    "        _dict[\"Company_Common_Name\"] = sector_data['Company_Common_Name'] \n",
    "        _dict[\"Economic_Sector_Name\"] = sector_data['TRBC_Economic_Sector_Name']\n",
    "        _dict[\"Business_Sector_Name\"] = sector_data['TRBC_Business_Sector_Name']\n",
    "        _dict[\"Industry_Group_Name\"] = sector_data['TRBC_Industry_Group_Name']\n",
    "        _dict[\"Industry_Name\"] = sector_data['TRBC_Industry_Name']\n",
    "        _dict[\"Activity_Name\"] = sector_data['TRBC_Activity_Name']\n",
    "\n",
    "        print(_dict)\n",
    "        \n",
    "        sequence_length = 12\n",
    "        X_val = np.array(df_last.select([\"lag_{}\".format(i) for i in range(1, sequence_length+1)]).collect())\n",
    "        next_6_months_predictions = model.predict(X_val).tolist()[0]\n",
    "       \n",
    "        \n",
    "        return render_template('index.html', ric=ric, prediction=next_6_months_predictions, sector_info=_dict)\n",
    "        #return jsonify(prediction=next_6_months_predictions)\n",
    "    \n",
    "    else:\n",
    "        return render_template('index.html')\n",
    "        print(\"else\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cad2a8-fabd-4ae7-9474-dff9d47d9659",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c034146f-b87c-4f46-9098-4ad758d0e30e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
