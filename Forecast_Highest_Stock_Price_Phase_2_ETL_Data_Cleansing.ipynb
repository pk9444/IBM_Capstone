{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7e6f91d-f6f8-4e68-bb67-10d289756f1d",
   "metadata": {},
   "source": [
    "# IBM Advanced Data Science Capstone: Forecasting Stock Prices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9a7791-0e12-4a37-9bea-75d565cf1918",
   "metadata": {},
   "source": [
    "## Extract-Transform-Load (ETL) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94779d6a-3421-4032-a9e6-4f8ea4c09c0c",
   "metadata": {},
   "source": [
    "### Data Cleansing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c18eac-0e7f-48c2-94bf-aa78d4a18af0",
   "metadata": {},
   "source": [
    " - Ensure that both the data files - main stocks dataset and the referential sectors dataset are in the same directory for feasible access. \n",
    " - Ensure that the any indexed or unnamed columns from the CSV file are duly dropped or not read into the spark dataframes. This add constraints in checking for data duplication and makes the dataset more noisy.\n",
    " - Ensure that the NULL values are correctly imputed for the stock prices and volumes. Based on the univariate analysis, all the features are not normally distributed and are sensitive to outliers. So, we will not impute them with the mean.\n",
    " - For imputation, first drop the records where Date is NULL as they are irrelevant. Then, among the non-NULL Date records, impute the numerical features with the median. \n",
    " - Ensure that all the features in the datasets are of the correct datatype.\n",
    " - Ensure to rename the column names of the features removing whitespaces, so that we can efficiently run SQL queries on the spark dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be1377a0-2e54-4249-a4a9-017802cb478b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# find and init the spark instance to ensure it is pip installed\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# set some HTML display setting \n",
    "from IPython.core.display import HTML\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))\n",
    "\n",
    "# import all the pyspark dependencies \n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, StringType, DateType\n",
    "from pyspark.sql.functions import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "# declare a spark object that we will run our spark SQL dataframes on \n",
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
    "\n",
    "# init a spark session \n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# import basic data analysis libraries  \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# import libraries for data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fce08b-f889-4ad5-b68b-af2111baaf10",
   "metadata": {},
   "source": [
    "#### Import the data files in spark dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2224df1b-d98c-46de-b056-cd9b133cae7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+----------+-----------+----------+-------+\n",
      "|Instrument|                Date|Price High|Price Close|Price Open| Volume|\n",
      "+----------+--------------------+----------+-----------+----------+-------+\n",
      "|   CBKG.DE|2019-01-02T00:00:00Z|     5.804|      5.765|     5.782|7221471|\n",
      "|   CBKG.DE|2019-01-03T00:00:00Z|      5.95|      5.802|     5.748|8064658|\n",
      "|   CBKG.DE|2019-01-04T00:00:00Z|     6.168|      6.143|      5.89|8772521|\n",
      "|   CBKG.DE|2019-01-07T00:00:00Z|     6.249|      6.182|     6.242|6781840|\n",
      "|   CBKG.DE|2019-01-08T00:00:00Z|      6.39|       6.33|     6.172|8472530|\n",
      "+----------+--------------------+----------+-----------+----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_stocks = spark.read.csv('kaggle_stock_data.csv', header=True).drop('_c0') # ensure that the index columns are not read as eatures\n",
    "data_stocks.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "757968b2-e1c2-4f6a-b955-75a5901fe1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------------------------+-------------------------+------------------------+--------------------+--------------------+-------------------------+-------------------------+------------------------+------------------+------------------+\n",
      "|Instrument| Company Common Name|TRBC Economic Sector Name|TRBC Business Sector Name|TRBC Industry Group Name|  TRBC Industry Name|  TRBC Activity Name|TRBC Economic Sector Code|TRBC Business Sector Code|TRBC Industry Group Code|TRBC Industry Code|TRBC Activity Code|\n",
      "+----------+--------------------+-------------------------+-------------------------+------------------------+--------------------+--------------------+-------------------------+-------------------------+------------------------+------------------+------------------+\n",
      "|   CBKG.DE|      Commerzbank AG|               Financials|     Banking & Investm...|        Banking Services|               Banks|         Banks (NEC)|                       55|                     5510|                  551010|          55101010|        5510101010|\n",
      "|  DTEGn.DE| Deutsche Telekom AG|               Technology|     Telecommunication...|    Telecommunication...|Integrated Teleco...|Integrated Teleco...|                       57|                     5740|                  574010|          57401010|        5740101010|\n",
      "|   ALVG.DE|          Allianz SE|               Financials|                Insurance|               Insurance|Multiline Insuran...|Multiline Insuran...|                       55|                     5530|                  553010|          55301010|        5530101010|\n",
      "|   MBGn.DE|Mercedes Benz Gro...|       Consumer Cyclicals|     Automobiles & Aut...|    Automobiles & Aut...|Auto & Truck Manu...|Auto & Truck Manu...|                       53|                     5310|                  531010|          53101010|        5310101010|\n",
      "|   SAPG.DE|              SAP SE|               Technology|     Software & IT Ser...|    Software & IT Ser...|            Software| Enterprise Software|                       57|                     5720|                  572010|          57201020|        5720102013|\n",
      "+----------+--------------------+-------------------------+-------------------------+------------------------+--------------------+--------------------+-------------------------+-------------------------+------------------------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_sectors = spark.read.csv('kaggle_stock_sector_information.csv', header=True).drop('_c0') # ensure that the index columns are not read as eatures\n",
    "data_sectors.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3096c39b-b3bc-4387-a36b-a19494b50b93",
   "metadata": {},
   "source": [
    "#### Appropriately format and/or rename the features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b812a91b-35ce-43d5-8864-7db91d44f05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function format_cols() compiled....\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "@format_cols() - a function to format the column names, replacing any whitespaces with '_'\n",
    "@params : df, input parameter that takes in the dataframe whose columns need to be formatted \n",
    "@return: output dataframe with the formatted column names \n",
    "'''\n",
    "def format_cols(df):\n",
    "    new_columns = [col(col_name).alias(col_name.replace(\" \", \"_\")) for col_name in df.columns]\n",
    "    return df.select(*new_columns)\n",
    "\n",
    "print(\"function format_cols() compiled....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8469a844-a426-45b2-b3fe-f20e04ba1374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+----------+-----------+----------+-------+\n",
      "|Instrument|                Date|Price_High|Price_Close|Price_Open| Volume|\n",
      "+----------+--------------------+----------+-----------+----------+-------+\n",
      "|   CBKG.DE|2019-01-02T00:00:00Z|     5.804|      5.765|     5.782|7221471|\n",
      "|   CBKG.DE|2019-01-03T00:00:00Z|      5.95|      5.802|     5.748|8064658|\n",
      "|   CBKG.DE|2019-01-04T00:00:00Z|     6.168|      6.143|      5.89|8772521|\n",
      "|   CBKG.DE|2019-01-07T00:00:00Z|     6.249|      6.182|     6.242|6781840|\n",
      "|   CBKG.DE|2019-01-08T00:00:00Z|      6.39|       6.33|     6.172|8472530|\n",
      "+----------+--------------------+----------+-----------+----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "format the dataframe for the stocks dataset \n",
    "'''\n",
    "data_stocks_formatted = format_cols(data_stocks)\n",
    "data_stocks_formatted.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63e2bc56-cf48-4664-a87b-e485c47d7631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------------------------+-------------------------+------------------------+--------------------+--------------------+-------------------------+-------------------------+------------------------+------------------+------------------+\n",
      "|Instrument| Company_Common_Name|TRBC_Economic_Sector_Name|TRBC_Business_Sector_Name|TRBC_Industry_Group_Name|  TRBC_Industry_Name|  TRBC_Activity_Name|TRBC_Economic_Sector_Code|TRBC_Business_Sector_Code|TRBC_Industry_Group_Code|TRBC_Industry_Code|TRBC_Activity_Code|\n",
      "+----------+--------------------+-------------------------+-------------------------+------------------------+--------------------+--------------------+-------------------------+-------------------------+------------------------+------------------+------------------+\n",
      "|   CBKG.DE|      Commerzbank AG|               Financials|     Banking & Investm...|        Banking Services|               Banks|         Banks (NEC)|                       55|                     5510|                  551010|          55101010|        5510101010|\n",
      "|  DTEGn.DE| Deutsche Telekom AG|               Technology|     Telecommunication...|    Telecommunication...|Integrated Teleco...|Integrated Teleco...|                       57|                     5740|                  574010|          57401010|        5740101010|\n",
      "|   ALVG.DE|          Allianz SE|               Financials|                Insurance|               Insurance|Multiline Insuran...|Multiline Insuran...|                       55|                     5530|                  553010|          55301010|        5530101010|\n",
      "|   MBGn.DE|Mercedes Benz Gro...|       Consumer Cyclicals|     Automobiles & Aut...|    Automobiles & Aut...|Auto & Truck Manu...|Auto & Truck Manu...|                       53|                     5310|                  531010|          53101010|        5310101010|\n",
      "|   SAPG.DE|              SAP SE|               Technology|     Software & IT Ser...|    Software & IT Ser...|            Software| Enterprise Software|                       57|                     5720|                  572010|          57201020|        5720102013|\n",
      "+----------+--------------------+-------------------------+-------------------------+------------------------+--------------------+--------------------+-------------------------+-------------------------+------------------------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "format the dataframe for the sectors info dataset \n",
    "'''\n",
    "data_sectors_formatted = format_cols(data_sectors)\n",
    "data_sectors_formatted.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a158b2-c5ec-43f8-95fe-6a2828dd6158",
   "metadata": {},
   "source": [
    "#### Assign the appropriate data type(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb591cab-e141-414e-9c51-ade2d16211c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Instrument', 'string'),\n",
       " ('Date', 'string'),\n",
       " ('Price_High', 'string'),\n",
       " ('Price_Close', 'string'),\n",
       " ('Price_Open', 'string'),\n",
       " ('Volume', 'string')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the data types in the dataframe currently \n",
    "data_stocks_formatted.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb4c56c-313b-481a-ace4-61b86d3ecbd8",
   "metadata": {},
   "source": [
    "We can notice that all of them are strings. Lets us convert them to the correct datatypes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e75dde79-dbe7-42f2-84d5-6a36e7edfb82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Instrument', 'string'),\n",
       " ('Date', 'date'),\n",
       " ('Price_High', 'double'),\n",
       " ('Price_Close', 'double'),\n",
       " ('Price_Open', 'double'),\n",
       " ('Volume', 'int')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assign the appropriate data type to each feature \n",
    "data_stocks_typeConverted = data_stocks_formatted.select(\n",
    "    col(\"Instrument\").cast(StringType()).alias(\"Instrument\"),\n",
    "    col(\"Date\").cast(DateType()).alias(\"Date\"),\n",
    "    col(\"Price_High\").cast(DoubleType()).alias(\"Price_High\"),\n",
    "    col(\"Price_Close\").cast(DoubleType()).alias(\"Price_Close\"),\n",
    "    col(\"Price_Open\").cast(DoubleType()).alias(\"Price_Open\"),\n",
    "    col(\"Volume\").cast(IntegerType()).alias(\"Volume\")\n",
    "    \n",
    ")\n",
    "data_stocks_typeConverted.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dbc05b-8b5c-41d0-b329-5dbd0af0e003",
   "metadata": {},
   "source": [
    "The datatypes for each feature have been appropriately converted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d1e8806-91ec-4ee6-b87b-e9388c6e438a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+-----------+----------+-------+\n",
      "|Instrument|      Date|Price_High|Price_Close|Price_Open| Volume|\n",
      "+----------+----------+----------+-----------+----------+-------+\n",
      "|   CBKG.DE|2019-01-02|     5.804|      5.765|     5.782|7221471|\n",
      "|   CBKG.DE|2019-01-03|      5.95|      5.802|     5.748|8064658|\n",
      "|   CBKG.DE|2019-01-04|     6.168|      6.143|      5.89|8772521|\n",
      "|   CBKG.DE|2019-01-07|     6.249|      6.182|     6.242|6781840|\n",
      "|   CBKG.DE|2019-01-08|      6.39|       6.33|     6.172|8472530|\n",
      "+----------+----------+----------+-----------+----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_stocks_typeConverted.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ea6ecb-e419-4780-9d82-735fab8d8437",
   "metadata": {},
   "source": [
    "#### Drop the duplicate records"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece670c4-4f54-4576-b935-4a0464f2d360",
   "metadata": {},
   "source": [
    "Recalling the findings from the EDA, there were a lot of duplicates in both datasets. Let us recount them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18797073-1f9a-49f1-8a69-5f9b3eda4d93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15257"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_stocks_typeConverted.groupBy(data_stocks_typeConverted.columns).count().filter(\"count > 1\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f67d20d-dd16-41f3-a628-49a563efb441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sectors_formatted.groupBy(data_sectors_formatted.columns).count().filter(\"count > 1\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bfbed2-0cb8-4b48-befb-c6f4fd6752a7",
   "metadata": {},
   "source": [
    "Now, let us remove these duplicates and reverify whether all the duplicates records have been dropped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8b5994a-d6cf-49bb-ba16-b806f8dc68aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "data_stocks_dropped = data_stocks_typeConverted.dropDuplicates()\n",
    "data_sectors_dropped = data_sectors_formatted.dropDuplicates()\n",
    "\n",
    "print(data_stocks_dropped.groupBy(data_stocks_dropped.columns).count().filter(\"count > 1\").count())\n",
    "print(data_sectors_dropped.groupBy(data_sectors_dropped.columns).count().filter(\"count > 1\").count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de49c72c-5f48-4951-bc19-037f265927d4",
   "metadata": {},
   "source": [
    "Now, there are no duplicate records in both stock and sector datasets. We can proceed to handle NULL values next. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e40140-c1d0-410b-aa0a-25a0504805b3",
   "metadata": {},
   "source": [
    "#### Handle NULL values "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cb84a0-9bf0-44c6-89ce-4540dfb5bc47",
   "metadata": {},
   "source": [
    "Based on the recommendations from the univariate analysis of the EDA: \n",
    "\n",
    "- Drop the records that do not a date i.e. the Date columns in NULL. \n",
    "- After dropping the NULL dates, impute the NULL entries in the numerical columns by the median for each company. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2d8d3d0-3f72-4c5e-b916-3f53432fbc5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function count_nulls() compiled ...\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "count_nulls() : a function count the number of in each column of a dataframe \n",
    "@param: an input dataframe on which we have to count the nulls in its columns \n",
    "@return: an output dataframe the contains the nulls counts for each column \n",
    "'''\n",
    "def count_nulls(df):\n",
    "    null_counts = df.select([sum(col(column).isNull().cast(\"int\")).alias(column) for column in df.columns])\n",
    "    return null_counts\n",
    "\n",
    "print(\"function count_nulls() compiled ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3b02753-8d31-463c-a305-84211b00cd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----------+-----------+----------+------+\n",
      "|Instrument|Date|Price_High|Price_Close|Price_Open|Volume|\n",
      "+----------+----+----------+-----------+----------+------+\n",
      "|         0|  72|       415|        380|       772| 57783|\n",
      "+----------+----+----------+-----------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# count the nulls in the initial cleaned \n",
    "count_nulls(data_stocks_dropped).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c67cee5-a2a2-461b-9afc-b5f1a3c02d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----------+-----------+----------+------+\n",
      "|Instrument|Date|Price_High|Price_Close|Price_Open|Volume|\n",
      "+----------+----+----------+-----------+----------+------+\n",
      "|         0|   0|       343|        308|       700| 57783|\n",
      "+----------+----+----------+-----------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop all those 72 NULL date records \n",
    "data_stocks_cleaned_1 = data_stocks_dropped.filter(col(\"Date\").isNotNull())\n",
    "count_nulls(data_stocks_cleaned_1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b827977-026c-4bbb-8fb0-f5dedcac3af6",
   "metadata": {},
   "source": [
    "Before we do the median imputations, lets extract the year and month from the Date column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29f63cf6-2149-4552-911a-3c61bebea0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+-----------+----------+--------+----+-----+\n",
      "|Instrument|      Date|Price_High|Price_Close|Price_Open|  Volume|Year|Month|\n",
      "+----------+----------+----------+-----------+----------+--------+----+-----+\n",
      "|   CBKG.DE|2020-03-30|     3.601|      3.338|     3.585|18925755|2020|    3|\n",
      "|   CBKG.DE|2022-02-21|     9.513|      9.153|       9.4|10191050|2022|    2|\n",
      "|   CBKG.DE|2022-11-03|     8.162|       8.08|     8.104| 4897012|2022|   11|\n",
      "|  DTEGn.DE|2020-10-20|    13.935|      13.68|      13.9|11136709|2020|   10|\n",
      "|  DTEGn.DE|2021-05-21|    17.224|      17.17|    17.004|10933087|2021|    5|\n",
      "+----------+----------+----------+-----------+----------+--------+----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract the 'year' and 'month' from the 'date_column'\n",
    "data_stocks_cleaned_1 = data_stocks_cleaned_1.withColumn(\"Year\", F.year(\"Date\"))\n",
    "data_stocks_cleaned_1 = data_stocks_cleaned_1.withColumn(\"Month\", F.month(\"Date\"))\n",
    "data_stocks_cleaned_1 = data_stocks_cleaned_1.filter(col('Year')>2018)\n",
    "data_stocks_cleaned_1.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0608a964-09c2-4537-9629-dbd1e3733ad4",
   "metadata": {},
   "source": [
    "Now, we will impute the NULLs with median for each RIC by the year and month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58aa6418-cd70-49ab-8d83-a17bb583fe28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a window specification for the DataFrame\n",
    "window_spec = Window.partitionBy(\"Instrument\",\"Year\", \"Month\")\n",
    "data_stocks_cleaned_1 = data_stocks_cleaned_1.sort([\"Instrument\", \"Year\", \"Month\"], ascending=[True, True, True])\n",
    "\n",
    "data_stocks_cleaned_1 = data_stocks_cleaned_1.withColumn(\"Price_Open\", F.when(data_stocks_cleaned_1[\"Price_Open\"].isNull(),\\\n",
    "                        F.expr(\"percentile(Price_Open, 0.5)\").over(window_spec)).otherwise(data_stocks_cleaned_1[\"Price_Open\"]))\n",
    "\n",
    "data_stocks_cleaned_1 = data_stocks_cleaned_1.withColumn(\"Price_Close\", F.when(data_stocks_cleaned_1[\"Price_Close\"].isNull(),\\\n",
    "                        F.expr(\"percentile(Price_Close, 0.5)\").over(window_spec)).otherwise(data_stocks_cleaned_1[\"Price_Close\"]))\n",
    "\n",
    "data_stocks_cleaned_1 = data_stocks_cleaned_1.withColumn(\"Price_High\", F.when(data_stocks_cleaned_1[\"Price_High\"].isNull(),\\\n",
    "                        F.expr(\"percentile(Price_High, 0.5)\").over(window_spec)).otherwise(data_stocks_cleaned_1[\"Price_High\"]))\n",
    "\n",
    "# data_stocks_cleaned_1 = data_stocks_cleaned_1.withColumn(\"Volume\", F.when(data_stocks_cleaned_1[\"Volume\"].isNull(),\\\n",
    "#                         F.expr(\"percentile(Volume, 0.5)\").over(window_spec)).otherwise(data_stocks_cleaned_1[\"Volume\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1dff9327-71c5-4df1-b647-3619c89f3d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----------+-----------+----------+------+----+-----+\n",
      "|Instrument|Date|Price_High|Price_Close|Price_Open|Volume|Year|Month|\n",
      "+----------+----+----------+-----------+----------+------+----+-----+\n",
      "|         0|   0|        22|          0|       379| 57783|   0|    0|\n",
      "+----------+----+----------+-----------+----------+------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count_nulls(data_stocks_cleaned_1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b39cda-1a43-4878-86bc-0fde7ad54bb1",
   "metadata": {},
   "source": [
    "As we can see although we performed some imputation, but still some NULLs exists. This may be noisy data it is better to drop them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "89636a6e-f2c2-4efe-b335-e3634dcc620f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stocks_cleaned_2 = data_stocks_cleaned_1.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4dea597f-4196-4932-bce7-836210d55077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----------+-----------+----------+------+----+-----+\n",
      "|Instrument|Date|Price_High|Price_Close|Price_Open|Volume|Year|Month|\n",
      "+----------+----+----------+-----------+----------+------+----+-----+\n",
      "|         0|   0|         0|          0|         0|     0|   0|    0|\n",
      "+----------+----+----------+-----------+----------+------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count_nulls(data_stocks_cleaned_2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fcbb13-cc4b-4e6a-b0af-6d856c8ca487",
   "metadata": {},
   "source": [
    "### Save the cleansed dataframes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21d757df-a080-4e51-ae0e-fe1011c114d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stocks_cleaned_2.toPandas().to_csv('kaggle_stock_data_cleansed.csv', header=True, index=False, mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3ac2acf-a31b-4af1-a429-a6c78a0a220c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sectors_dropped.toPandas().to_csv('kaggle_stock_sector_information_cleansed.csv', header=True, index=False, mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95ca298-f6dd-4f14-9169-2ee695f708c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
